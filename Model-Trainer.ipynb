{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "## BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install PyPDF2 pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Fonction pour extraire le texte d'un PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error reading {pdf_path}: {str(e)}\"\n",
    "\n",
    "# Dossier contenant les PDF\n",
    "directory = \"BDD\"\n",
    "data = []\n",
    "\n",
    "# Vérifier si le dossier existe\n",
    "if not os.path.exists(directory):\n",
    "    print(f\"Le dossier '{directory}' n'existe pas.\")\n",
    "else:\n",
    "    # Parcourir tous les fichiers dans le dossier\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(directory, filename)\n",
    "            #print(f\"Extraction du contenu de : {filename}\")\n",
    "            extracted_text = extract_text_from_pdf(pdf_path)\n",
    "            data.append([filename, extracted_text])\n",
    "\n",
    "    # Créer un DataFrame pour manipuler les données\n",
    "    pdf_dataframe = pd.DataFrame(data, columns=[\"Title\", \"Content\"])\n",
    "\n",
    "    # Nettoyer les titres\n",
    "    pdf_dataframe['Title'] = pdf_dataframe['Title'].str.replace(r'\\d+', '', regex=True)  # Supprimer les nombres\n",
    "    pdf_dataframe['Title'] = pdf_dataframe['Title'].str.replace(r'\\.pdf', '', regex=True)  # Supprimer \".pdf\"\n",
    "    pdf_dataframe['Title'] = pdf_dataframe['Title'].str.replace(r'[_-]', '', regex=True)  # Supprimer \"_\" et \"-\"\n",
    "\n",
    "    # Remplacer les valeurs spécifiques dans la colonne Title\n",
    "    pdf_dataframe['Title'] = pdf_dataframe['Title'].replace({\n",
    "        'assignation': 0,\n",
    "        'assignationsansVices': 1,\n",
    "        'NotificationVice': 2,\n",
    "        'Notification': 3\n",
    "    })\n",
    "\n",
    "    # Afficher la répartition des classes avant équilibrage\n",
    "    print(\"Répartition des classes avant équilibrage :\")\n",
    "    print(pdf_dataframe['Title'].value_counts())\n",
    "\n",
    "    # Séparer les classes majoritaires et minoritaires\n",
    "    majority = pdf_dataframe[pdf_dataframe['Title'] == 0]\n",
    "    minority_1 = pdf_dataframe[pdf_dataframe['Title'] == 1]\n",
    "    minority_2 = pdf_dataframe[pdf_dataframe['Title'] == 2]\n",
    "    minority_3 = pdf_dataframe[pdf_dataframe['Title'] == 3]\n",
    "\n",
    "    # Suréchantillonner les classes minoritaires\n",
    "    minority_1_upsampled = resample(minority_1, \n",
    "                                    replace=True,     # Permet le suréchantillonnage\n",
    "                                    n_samples=len(majority),  # Faire correspondre la taille de la classe majoritaire\n",
    "                                    random_state=123)\n",
    "    minority_2_upsampled = resample(minority_2, \n",
    "                                    replace=True, \n",
    "                                    n_samples=len(majority), \n",
    "                                    random_state=123)\n",
    "    minority_3_upsampled = resample(minority_3, \n",
    "                                    replace=True, \n",
    "                                    n_samples=len(majority), \n",
    "                                    random_state=123)\n",
    "\n",
    "    # Fusionner toutes les classes après le suréchantillonnage\n",
    "    data_balanced = pd.concat([majority, minority_1_upsampled, minority_2_upsampled, minority_3_upsampled])\n",
    "\n",
    "    # Sauvegarder le contenu équilibré dans un fichier CSV\n",
    "    output_file = \"pdf_contents_final.csv\"\n",
    "    data_balanced.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Le contenu équilibré a été sauvegardé dans '{output_file}'.\")\n",
    "\n",
    "    # Afficher la répartition des classes après équilibrage\n",
    "    print(\"Répartition des classes après équilibrage :\")\n",
    "    print(data_balanced['Title'].value_counts())\n",
    "\n",
    "    # Afficher les premières lignes du DataFrame équilibré\n",
    "    #print(\"Aperçu des données équilibrées :\")\n",
    "    #print(data_balanced.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legal BERT V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c53b2801f09487d97c65fa4112307c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6170464754104614, 'eval_runtime': 5.4151, 'eval_samples_per_second': 13.665, 'eval_steps_per_second': 1.847, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92e338b9ece47aea11715c3a4a638ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4249761998653412, 'eval_runtime': 5.6577, 'eval_samples_per_second': 13.08, 'eval_steps_per_second': 1.768, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ab977dfa114cad981382ba9662c944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3963528573513031, 'eval_runtime': 5.7534, 'eval_samples_per_second': 12.862, 'eval_steps_per_second': 1.738, 'epoch': 3.0}\n",
      "{'train_runtime': 217.9124, 'train_samples_per_second': 4.047, 'train_steps_per_second': 0.509, 'train_loss': 0.6449106577280406, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb149940046b42fcb07b8dc2cb431e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3286d859a9448b98bf67832ee2d2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 3 1 2 1 1 1 1 2 0 1 0 0 2 0 2 3 0 0 3 0 2 2 2 2 2 3 3 0 1 0 3 1 1 0\n",
      " 1 3 1 2 2 0 2 1 3 0 0 1 0 2 3 3 3 1 3 1 0 2 1 2 2 0 0 3 1 2 1 2 0 1 2 3 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./legal_bert_model/tokenizer_config.json',\n",
       " './legal_bert_model/special_tokens_map.json',\n",
       " './legal_bert_model/vocab.txt',\n",
       " './legal_bert_model/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_csv('pdf_contents_final.csv')\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Content'], df['Title'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Charger le tokenizer de LegalBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "# Tokenisation des données\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_encodings = tokenize_function(X_train.tolist())\n",
    "test_encodings = tokenize_function(X_test.tolist())\n",
    "\n",
    "# Créer un Dataset personnalisé pour BERT\n",
    "class LegalBERTDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "# Créer les datasets pour l'entraînement et l'évaluation\n",
    "train_dataset = LegalBERTDataset(train_encodings, y_train)\n",
    "test_dataset = LegalBERTDataset(test_encodings, y_test)\n",
    "\n",
    "# Charger le modèle pré-entraîné de LegalBERT pour la classification\n",
    "model = BertForSequenceClassification.from_pretrained('nlpaueb/legal-bert-base-uncased', num_labels=4)\n",
    "\n",
    "# Arguments d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Répertoire de sortie\n",
    "    evaluation_strategy=\"epoch\",     # Stratégie d'évaluation par époque\n",
    "    learning_rate=2e-5,              # Taux d'apprentissage\n",
    "    per_device_train_batch_size=8,   # Taille du batch pour l'entraînement\n",
    "    per_device_eval_batch_size=8,    # Taille du batch pour l'évaluation\n",
    "    num_train_epochs=3,              # Nombre d'époques\n",
    "    weight_decay=0.01,               # Décroissance du poids\n",
    "    logging_dir='./logs',            # Répertoire des logs\n",
    ")\n",
    "\n",
    "# Initialiser le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "trainer.train()\n",
    "\n",
    "# Évaluer le modèle\n",
    "trainer.evaluate()\n",
    "\n",
    "# Prédictions sur le jeu de test\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Afficher les résultats des prédictions\n",
    "print(predictions.predictions.argmax(axis=-1))\n",
    "\n",
    "# Sauvegarder le modèle et le tokenizer\n",
    "model.save_pretrained('./legal_bert_model')\n",
    "tokenizer.save_pretrained('./legal_bert_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemples de prédictions comparées aux véritables titres :\n",
      "   Title  Predicted_Label\n",
      "0      0                0\n",
      "1      0                0\n",
      "2      0                0\n",
      "3      0                0\n",
      "4      0                0\n",
      "Précision du modèle : 89.40%\n",
      "\n",
      "Matrice de confusion :\n",
      "[[63 29  0  0]\n",
      " [10 82  0  0]\n",
      " [ 0  0 92  0]\n",
      " [ 0  0  0 92]]\n",
      "\n",
      "Nombre de bonnes réponses : 329\n",
      "Nombre de mauvaises réponses : 39\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_csv('pdf_contents_final.csv')\n",
    "\n",
    "# Charger le modèle et le tokenizer sauvegardés\n",
    "model = BertForSequenceClassification.from_pretrained('./legal_bert_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./legal_bert_model')\n",
    "\n",
    "# Fonction pour prédire la classe d'un texte\n",
    "def predict(texts):\n",
    "    # Tokeniser les nouveaux textes\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    # Obtenir les prédictions du modèle\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Obtenir les indices des classes prédominantes\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions\n",
    "\n",
    "# Tester le modèle avec le contenu du dataset\n",
    "new_texts = df['Content'].tolist()  # Liste des contenus des PDFs\n",
    "predicted_labels = predict(new_texts)\n",
    "\n",
    "# Ajouter les prédictions dans le dataframe\n",
    "df['Predicted_Label'] = predicted_labels.numpy()\n",
    "\n",
    "# Afficher les résultats avec les titres\n",
    "print(\"Exemples de prédictions comparées aux véritables titres :\")\n",
    "print(df[['Title', 'Predicted_Label']].head())\n",
    "\n",
    "# Calcul de la précision\n",
    "accuracy = accuracy_score(df['Title'], df['Predicted_Label'])\n",
    "print(f\"Précision du modèle : {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix pour visualiser les bonnes et mauvaises prédictions\n",
    "conf_matrix = confusion_matrix(df['Title'], df['Predicted_Label'])\n",
    "print(\"\\nMatrice de confusion :\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Nombre de bonnes réponses\n",
    "correct_predictions = (df['Title'] == df['Predicted_Label']).sum()\n",
    "print(f\"\\nNombre de bonnes réponses : {correct_predictions}\")\n",
    "\n",
    "# Nombre de mauvaises réponses\n",
    "incorrect_predictions = len(df) - correct_predictions\n",
    "print(f\"Nombre de mauvaises réponses : {incorrect_predictions}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
